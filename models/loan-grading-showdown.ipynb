{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Can I Grade Loans Better Than LendingClub?\n\n<div style=\"font-size: 16px; font-weight: 500; line-height: 1.4; font-style: italic; margin-bottom: 1em;\">\n  Pitting My Neural Network Against a Corporate Benchmark\n</div>\n\n1. **[Introduction](#Introduction)**\n2. **[Ground rules](#Ground-rules)**\n3. **[Test metric](#Test-metric)**\n4. **[LendingClub's turn](#LendingClub's-turn)**\n5. **[My turn](#My-turn)**\n6. **[Victory!](#Victory!)**\n7. **[Further reading](#Further-reading)**\n\n## Introduction\n\nIn case you missed it, I [built a neural network to predict loan risk](https://www.kaggle.com/tywmick/building-a-neural-network-to-predict-loan-risk) using a [public dataset](https://www.kaggle.com/wordsforthewise/lending-club) from [LendingClub](https://www.lendingclub.com/). Then I built a [public API](https://tywmick.pythonanywhere.com/) to serve the model's predictions. That's nice and all, but&hellip; how _good_ is my model?\n\nToday I'm going to put it to the test, pitting it against the risk models of the very institution who issued those loans. That's right, LendingClub included their own calculated loan grades (and sub-grades) in the dataset, so all the pieces are in place for the most thrilling risk modeling smackdown of the ~~century~~ week. May the best algorithm win!"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import joblib\n\nloans = joblib.load(\"loans_for_eval.joblib\")\nloans.shape",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 1,
          "data": {
            "text/plain": "(1110171, 70)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "loans.head()",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 2,
          "data": {
            "text/plain": "   loan_amnt       term emp_length home_ownership  annual_inc  \\\n0     3600.0  36 months  10+ years       MORTGAGE     55000.0   \n1    24700.0  36 months  10+ years       MORTGAGE     65000.0   \n2    20000.0  60 months  10+ years       MORTGAGE     63000.0   \n4    10400.0  60 months    3 years       MORTGAGE    104433.0   \n5    11950.0  36 months    4 years           RENT     34000.0   \n\n              purpose    dti  delinq_2yrs  cr_hist_age_mths  fico_range_low  \\\n0  debt_consolidation   5.91          0.0             148.0           675.0   \n1      small_business  16.06          1.0             192.0           715.0   \n2    home_improvement  10.78          0.0             184.0           695.0   \n4      major_purchase  25.37          1.0             210.0           695.0   \n5  debt_consolidation  10.20          0.0             338.0           690.0   \n\n   ...  tax_liens  tot_hi_cred_lim  total_bal_ex_mort  total_bc_limit  \\\n0  ...        0.0         178050.0             7746.0          2400.0   \n1  ...        0.0         314017.0            39475.0         79300.0   \n2  ...        0.0         218418.0            18696.0          6200.0   \n4  ...        0.0         439570.0            95768.0         20300.0   \n5  ...        0.0          16900.0            12798.0          9400.0   \n\n   total_il_high_credit_limit  fraction_recovered   issue_d  grade  sub_grade  \\\n0                     13734.0                 1.0  Dec-2015      C         C4   \n1                     24667.0                 1.0  Dec-2015      C         C1   \n2                     14877.0                 1.0  Dec-2015      B         B4   \n4                     88097.0                 1.0  Dec-2015      F         F1   \n5                      4000.0                 1.0  Dec-2015      C         C3   \n\n   expected_return  \n0          4429.08  \n1         29530.08  \n2         25959.60  \n4         17394.60  \n5         14586.48  \n\n[5 rows x 70 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>loan_amnt</th>\n      <th>term</th>\n      <th>emp_length</th>\n      <th>home_ownership</th>\n      <th>annual_inc</th>\n      <th>purpose</th>\n      <th>dti</th>\n      <th>delinq_2yrs</th>\n      <th>cr_hist_age_mths</th>\n      <th>fico_range_low</th>\n      <th>...</th>\n      <th>tax_liens</th>\n      <th>tot_hi_cred_lim</th>\n      <th>total_bal_ex_mort</th>\n      <th>total_bc_limit</th>\n      <th>total_il_high_credit_limit</th>\n      <th>fraction_recovered</th>\n      <th>issue_d</th>\n      <th>grade</th>\n      <th>sub_grade</th>\n      <th>expected_return</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3600.0</td>\n      <td>36 months</td>\n      <td>10+ years</td>\n      <td>MORTGAGE</td>\n      <td>55000.0</td>\n      <td>debt_consolidation</td>\n      <td>5.91</td>\n      <td>0.0</td>\n      <td>148.0</td>\n      <td>675.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>178050.0</td>\n      <td>7746.0</td>\n      <td>2400.0</td>\n      <td>13734.0</td>\n      <td>1.0</td>\n      <td>Dec-2015</td>\n      <td>C</td>\n      <td>C4</td>\n      <td>4429.08</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>24700.0</td>\n      <td>36 months</td>\n      <td>10+ years</td>\n      <td>MORTGAGE</td>\n      <td>65000.0</td>\n      <td>small_business</td>\n      <td>16.06</td>\n      <td>1.0</td>\n      <td>192.0</td>\n      <td>715.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>314017.0</td>\n      <td>39475.0</td>\n      <td>79300.0</td>\n      <td>24667.0</td>\n      <td>1.0</td>\n      <td>Dec-2015</td>\n      <td>C</td>\n      <td>C1</td>\n      <td>29530.08</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>20000.0</td>\n      <td>60 months</td>\n      <td>10+ years</td>\n      <td>MORTGAGE</td>\n      <td>63000.0</td>\n      <td>home_improvement</td>\n      <td>10.78</td>\n      <td>0.0</td>\n      <td>184.0</td>\n      <td>695.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>218418.0</td>\n      <td>18696.0</td>\n      <td>6200.0</td>\n      <td>14877.0</td>\n      <td>1.0</td>\n      <td>Dec-2015</td>\n      <td>B</td>\n      <td>B4</td>\n      <td>25959.60</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>10400.0</td>\n      <td>60 months</td>\n      <td>3 years</td>\n      <td>MORTGAGE</td>\n      <td>104433.0</td>\n      <td>major_purchase</td>\n      <td>25.37</td>\n      <td>1.0</td>\n      <td>210.0</td>\n      <td>695.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>439570.0</td>\n      <td>95768.0</td>\n      <td>20300.0</td>\n      <td>88097.0</td>\n      <td>1.0</td>\n      <td>Dec-2015</td>\n      <td>F</td>\n      <td>F1</td>\n      <td>17394.60</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>11950.0</td>\n      <td>36 months</td>\n      <td>4 years</td>\n      <td>RENT</td>\n      <td>34000.0</td>\n      <td>debt_consolidation</td>\n      <td>10.20</td>\n      <td>0.0</td>\n      <td>338.0</td>\n      <td>690.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>16900.0</td>\n      <td>12798.0</td>\n      <td>9400.0</td>\n      <td>4000.0</td>\n      <td>1.0</td>\n      <td>Dec-2015</td>\n      <td>C</td>\n      <td>C3</td>\n      <td>14586.48</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 70 columns</p>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Ground rules\n\nThis is going to be a clean fight—my model won't use any data LendingClub wouldn't have access to at the point they calculate a loan's grade (including the grade itself).\n\nI'm going to sort the dataset chronologically (using the `issue_d` column, the month and year the loan was issued) and split it into two parts. The first 80% I'll use for training my competition model, and I'll compare performance on the last 20%."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import train_test_split\n\nloans[\"date\"] = loans[\"issue_d\"].astype(\"datetime64[ns]\")\nloans.sort_values(\"date\", axis=\"index\", inplace=True, kind=\"mergesort\")\n\ntrain, test = train_test_split(loans, test_size=0.2, shuffle=False)\ntrain, test = train.copy(), test.copy()\nprint(f\"The test set contains {len(test):,} loans.\")",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": "The test set contains 222,035 loans.\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "At the earlier end of the test set my model may have a slight informational advantage, having been trained on a few loans that may not have closed yet at the point LendingClub was grading those ones. On the other hand, LendingClub may have a slight informational advantage on the later end of the test set, since they would have known the outcomes of some loans on the earlier end of the test set by that time.\n\nI have to give credit to Michael Wurm, by the way, for [the idea](https://towardsdatascience.com/intelligent-loan-selection-for-peer-to-peer-lending-575dfa2573cb#fac8) of comparing my model's performance to LendingClub's loan grades, but my approach is pretty different. I'm not trying to simulate the performance of an investment portfolio; I'm just evaluating how well my predictions of simple risk compare.\n\n## Test metric\n\nThe test: who can pick the best set of grade A loans, judged on the basis of the independent variable from [my last notebook](https://www.kaggle.com/tywmick/building-a-neural-network-to-predict-loan-risk), the fraction of an expected loan return that a prospective borrower will pay back (which I engineered as `fraction_recovered`).\n\nLendingClub will take the plate first. I'll gather all their grade A loans from the test set, count them, and calculate their average `fraction_recovered`. That average will be the metric my model has to beat.\n\nThen I'll train my model on the training set using the same [pipeline and parameters](https://www.kaggle.com/tywmick/building-a-neural-network-to-predict-loan-risk#Building-the-neural-networks) I settled on in my last notebook. Once it's trained, I'll use it to make predictions on the test set, then gather the number of top predictions equal to the number of LendingClub's grade A loans. Finally, I'll calculate the same average of `fraction_recovered` on that subset, and we'll have ourselves a winner!\n\n## LendingClub's turn"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from statistics import mean\n\nlc_grade_a = test[test[\"grade\"] == \"A\"]\nprint(f\"LendingClub gave {len(lc_grade_a):,} loans in the test set an A grade.\")\n\nprint(\"\\nAverage `fraction_recovered` on LendingClub's grade A loans:\")\nprint(format(mean(lc_grade_a[\"fraction_recovered\"]), \".5f\"))",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": "LendingClub gave 38,779 loans in the test set an A grade.\n\nAverage `fraction_recovered` on LendingClub's grade A loans:\n0.96021\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "That's a pretty high percentage. I'm a bit nervous.\n\n## My turn\n\nFirst, I'll copy over my `run_pipeline` function from [my previous notebook](https://www.kaggle.com/tywmick/building-a-neural-network-to-predict-loan-risk#Building-the-neural-networks). I'll hide the input here since it isn't new information, but feel free to take a peek by clicking the \"Input\" button."
    },
    {
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import train_test_split\nfrom sklearn_pandas import DataFrameMapper\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\nfrom tensorflow.keras import Sequential, Input\nfrom tensorflow.keras.layers import Dense, Dropout\n\n\ndef run_pipeline(\n    data,\n    onehot_cols,\n    ordinal_cols,\n    batch_size,\n    validate=True,\n):\n    X = data.drop(columns=[\"fraction_recovered\"])\n    y = data[\"fraction_recovered\"]\n    X_train, X_valid, y_train, y_valid = (\n        train_test_split(X, y, test_size=0.2, random_state=0)\n        if validate\n        else (X, None, y, None)\n    )\n\n    transformer = DataFrameMapper(\n        [\n            (onehot_cols, OneHotEncoder(drop=\"if_binary\")),\n            (\n                list(ordinal_cols.keys()),\n                OrdinalEncoder(categories=list(ordinal_cols.values())),\n            ),\n        ],\n        default=StandardScaler(),\n    )\n\n    X_train = transformer.fit_transform(X_train)\n    X_valid = transformer.transform(X_valid) if validate else None\n\n    input_nodes = X_train.shape[1]\n    output_nodes = 1\n\n    model = Sequential()\n    model.add(Input((input_nodes,)))\n    model.add(Dense(64, activation=\"relu\"))\n    model.add(Dropout(0.3, seed=0))\n    model.add(Dense(32, activation=\"relu\"))\n    model.add(Dropout(0.3, seed=1))\n    model.add(Dense(16, activation=\"relu\"))\n    model.add(Dropout(0.3, seed=2))\n    model.add(Dense(output_nodes))\n    model.compile(optimizer=\"adam\", loss=\"mean_squared_logarithmic_error\")\n\n    history = model.fit(\n        X_train,\n        y_train,\n        batch_size=batch_size,\n        epochs=100,\n        validation_data=(X_valid, y_valid) if validate else None,\n        verbose=2,\n    )\n\n    return history.history, model, transformer\n\n\nonehot_cols = [\"term\", \"application_type\", \"home_ownership\", \"purpose\"]\nordinal_cols = {\n    \"emp_length\": [\n        \"< 1 year\",\n        \"1 year\",\n        \"2 years\",\n        \"3 years\",\n        \"4 years\",\n        \"5 years\",\n        \"6 years\",\n        \"7 years\",\n        \"8 years\",\n        \"9 years\",\n        \"10+ years\",\n    ]\n}",
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Train the model\n_, model, transformer = run_pipeline(\n    train.drop(columns=[\"issue_d\", \"date\", \"grade\", \"sub_grade\", \"expected_return\"]),\n    onehot_cols,\n    ordinal_cols,\n    batch_size=128,\n    validate=False,\n)\n\n# Make predictions\nX_test = transformer.transform(\n    test.drop(\n        columns=[\n            \"fraction_recovered\",\n            \"issue_d\",\n            \"date\",\n            \"grade\",\n            \"sub_grade\",\n            \"expected_return\",\n        ]\n    )\n)\ntest[\"model_predictions\"] = model.predict(X_test)\n\n# Gather top predictions\ntest_sorted = test.sort_values(\"model_predictions\", axis=\"index\", ascending=False)\nty_grade_a = test_sorted.iloc[0:len(lc_grade_a)]\n\n# Display results\nprint(\"\\nAverage `fraction_recovered` on Ty's grade A loans:\")\nprint(format(mean(ty_grade_a[\"fraction_recovered\"]), \".5f\"))",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Epoch 1/100\n6939/6939 - 13s - loss: 0.0249\nEpoch 2/100\n6939/6939 - 13s - loss: 0.0204\nEpoch 3/100\n6939/6939 - 13s - loss: 0.0202\nEpoch 4/100\n6939/6939 - 13s - loss: 0.0202\nEpoch 5/100\n6939/6939 - 13s - loss: 0.0202\nEpoch 6/100\n6939/6939 - 14s - loss: 0.0201\nEpoch 7/100\n6939/6939 - 14s - loss: 0.0201\nEpoch 8/100\n6939/6939 - 14s - loss: 0.0201\nEpoch 9/100\n6939/6939 - 13s - loss: 0.0201\nEpoch 10/100\n6939/6939 - 12s - loss: 0.0201\nEpoch 11/100\n6939/6939 - 13s - loss: 0.0201\nEpoch 12/100\n6939/6939 - 13s - loss: 0.0201\nEpoch 13/100\n6939/6939 - 13s - loss: 0.0201\nEpoch 14/100\n6939/6939 - 13s - loss: 0.0201\nEpoch 15/100\n6939/6939 - 12s - loss: 0.0201\nEpoch 16/100\n6939/6939 - 12s - loss: 0.0201\nEpoch 17/100\n6939/6939 - 13s - loss: 0.0200\nEpoch 18/100\n6939/6939 - 13s - loss: 0.0200\nEpoch 19/100\n6939/6939 - 13s - loss: 0.0200\nEpoch 20/100\n6939/6939 - 14s - loss: 0.0200\nEpoch 21/100\n6939/6939 - 13s - loss: 0.0200\nEpoch 22/100\n6939/6939 - 13s - loss: 0.0200\nEpoch 23/100\n6939/6939 - 12s - loss: 0.0200\nEpoch 24/100\n6939/6939 - 12s - loss: 0.0200\nEpoch 25/100\n6939/6939 - 12s - loss: 0.0200\nEpoch 26/100\n6939/6939 - 13s - loss: 0.0200\nEpoch 27/100\n6939/6939 - 13s - loss: 0.0200\nEpoch 28/100\n6939/6939 - 13s - loss: 0.0200\nEpoch 29/100\n6939/6939 - 13s - loss: 0.0200\nEpoch 30/100\n6939/6939 - 13s - loss: 0.0200\nEpoch 31/100\n6939/6939 - 15s - loss: 0.0200\nEpoch 32/100\n6939/6939 - 13s - loss: 0.0200\nEpoch 33/100\n6939/6939 - 12s - loss: 0.0200\nEpoch 34/100\n6939/6939 - 13s - loss: 0.0200\nEpoch 35/100\n6939/6939 - 13s - loss: 0.0200\nEpoch 36/100\n6939/6939 - 13s - loss: 0.0200\nEpoch 37/100\n6939/6939 - 13s - loss: 0.0200\nEpoch 38/100\n6939/6939 - 13s - loss: 0.0200\nEpoch 39/100\n6939/6939 - 13s - loss: 0.0200\nEpoch 40/100\n6939/6939 - 13s - loss: 0.0200\nEpoch 41/100\n6939/6939 - 13s - loss: 0.0200\nEpoch 42/100\n6939/6939 - 13s - loss: 0.0200\nEpoch 43/100\n6939/6939 - 14s - loss: 0.0200\nEpoch 44/100\n6939/6939 - 13s - loss: 0.0200\nEpoch 45/100\n6939/6939 - 13s - loss: 0.0200\nEpoch 46/100\n6939/6939 - 13s - loss: 0.0200\nEpoch 47/100\n6939/6939 - 13s - loss: 0.0200\nEpoch 48/100\n6939/6939 - 13s - loss: 0.0200\nEpoch 49/100\n6939/6939 - 13s - loss: 0.0200\nEpoch 50/100\n6939/6939 - 13s - loss: 0.0200\nEpoch 51/100\n6939/6939 - 13s - loss: 0.0200\nEpoch 52/100\n6939/6939 - 13s - loss: 0.0200\nEpoch 53/100\n6939/6939 - 13s - loss: 0.0200\nEpoch 54/100\n6939/6939 - 14s - loss: 0.0200\nEpoch 55/100\n6939/6939 - 14s - loss: 0.0200\nEpoch 56/100\n6939/6939 - 13s - loss: 0.0200\nEpoch 57/100\n6939/6939 - 13s - loss: 0.0200\nEpoch 58/100\n6939/6939 - 13s - loss: 0.0200\nEpoch 59/100\n6939/6939 - 13s - loss: 0.0200\nEpoch 60/100\n6939/6939 - 13s - loss: 0.0200\nEpoch 61/100\n6939/6939 - 13s - loss: 0.0200\nEpoch 62/100\n6939/6939 - 13s - loss: 0.0200\nEpoch 63/100\n6939/6939 - 13s - loss: 0.0200\nEpoch 64/100\n6939/6939 - 13s - loss: 0.0200\nEpoch 65/100\n6939/6939 - 12s - loss: 0.0200\nEpoch 66/100\n6939/6939 - 13s - loss: 0.0200\nEpoch 67/100\n6939/6939 - 14s - loss: 0.0200\nEpoch 68/100\n6939/6939 - 13s - loss: 0.0200\nEpoch 69/100\n6939/6939 - 13s - loss: 0.0200\nEpoch 70/100\n6939/6939 - 13s - loss: 0.0200\nEpoch 71/100\n6939/6939 - 13s - loss: 0.0200\nEpoch 72/100\n6939/6939 - 13s - loss: 0.0200\nEpoch 73/100\n6939/6939 - 13s - loss: 0.0200\nEpoch 74/100\n6939/6939 - 13s - loss: 0.0200\nEpoch 75/100\n6939/6939 - 13s - loss: 0.0200\nEpoch 76/100\n6939/6939 - 13s - loss: 0.0200\nEpoch 77/100\n6939/6939 - 13s - loss: 0.0200\nEpoch 78/100\n6939/6939 - 13s - loss: 0.0200\nEpoch 79/100\n6939/6939 - 14s - loss: 0.0200\nEpoch 80/100\n6939/6939 - 13s - loss: 0.0200\nEpoch 81/100\n6939/6939 - 13s - loss: 0.0200\nEpoch 82/100\n6939/6939 - 13s - loss: 0.0200\nEpoch 83/100\n6939/6939 - 13s - loss: 0.0200\nEpoch 84/100\n6939/6939 - 12s - loss: 0.0200\nEpoch 85/100\n6939/6939 - 13s - loss: 0.0200\nEpoch 86/100\n6939/6939 - 13s - loss: 0.0200\nEpoch 87/100\n6939/6939 - 13s - loss: 0.0200\nEpoch 88/100\n6939/6939 - 13s - loss: 0.0200\nEpoch 89/100\n6939/6939 - 13s - loss: 0.0200\nEpoch 90/100\n6939/6939 - 13s - loss: 0.0200\nEpoch 91/100\n6939/6939 - 14s - loss: 0.0200\nEpoch 92/100\n6939/6939 - 13s - loss: 0.0200\nEpoch 93/100\n6939/6939 - 13s - loss: 0.0200\nEpoch 94/100\n6939/6939 - 13s - loss: 0.0200\nEpoch 95/100\n6939/6939 - 13s - loss: 0.0200\nEpoch 96/100\n6939/6939 - 13s - loss: 0.0200\nEpoch 97/100\n6939/6939 - 13s - loss: 0.0200\nEpoch 98/100\n6939/6939 - 13s - loss: 0.0200\nEpoch 99/100\n6939/6939 - 13s - loss: 0.0200\nEpoch 100/100\n6939/6939 - 13s - loss: 0.0200\n\nAverage `fraction_recovered` on Ty's grade A loans:\n0.96166\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Victory!\n\nPhew, that was a close one! My win might be too small to be statistically significant, but hey, it's cool seeing that I can keep up with LendingClub's best and brightest.\n\n## Further reading\n\n- [Natural Language Processing for Loan Risk](https://www.kaggle.com/tywmick/natural-language-processing-for-loan-risk)\n\n-----\n\nWhat I'd really like to know now is what quantitative range of estimated risk each LendingClub grade and sub-grade corresponds to, but it looks like [that's proprietary](https://www.lendingclub.com/foliofn/rateDetail.action). Does anyone know if loans grades generally correspond to certain percentage ranges like letter grades in academic classes? If not, have any ideas for better benchmarks I could use to evaluate my model's performance? Go ahead and chime in in the comments below."
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
